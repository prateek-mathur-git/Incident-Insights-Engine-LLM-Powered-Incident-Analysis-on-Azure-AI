import os
import json
import pandas as pd
from openai import AzureOpenAI
from tqdm import tqdm

# --- 1. Set Up Azure OpenAI Connection ---
# CRITICAL: Replace these placeholders with your actual Azure OpenAI values
AZURE_ENDPOINT = os.environ.get("AZURE_ENDPOINT", "") # <YOUR AZURE ENDPOINT HERE>
AZURE_API_KEY = os.environ.get("AZURE_API_KEY", "") # <YOUR AZURE API KEY HERE>
DEPLOYMENT_NAME = os.environ.get("DEPLOYMENT_NAME", "gpt-4o-mini") # e.g., gpt-4o-mini

# Initialize the AzureOpenAI Client
try:
    client = AzureOpenAI(
        azure_endpoint=AZURE_ENDPOINT,
        api_key=AZURE_API_KEY,
        api_version="2024-02-15-preview"
    )
except Exception as e:
    print(f"Error initializing AzureOpenAI client. Check your credentials: {e}")
    # Exit or handle gracefully if credentials are bad
    # exit()

# --- 2. Load Incident Dataset ---
# CRITICAL: Ensure the 'incidents.ndjson' file exists in the same directory
input_file = "incident_data.ndjson"

try:
    records = []
    with open(input_file, "r") as f:
        for line in f:
            if line.strip():
                records.append(json.loads(line))
    df = pd.DataFrame(records)
    print(f"Successfully loaded {len(df)} incidents from {input_file}.")
    print("\n--- Sample Data ---\n", df.head())
except FileNotFoundError:
    print(f"ERROR: Input file '{input_file}' not found. Please create it.")
    df = pd.DataFrame()
except Exception as e:
    print(f"ERROR during file loading: {e}")
    df = pd.DataFrame()

# --- 3. Define LLM Prompt Template ---
PROMPT_TEMPLATE = """You are an SRE/Operations expert.
Analyze the incident below and extract:
1. Root Cause (precise)
2. Recommended Fix (actionable)
3. Risk of Reoccurrence (Low/Medium/High)
4. Summary in 1 short sentence.

Respond ONLY in JSON, ensuring the output is valid and complete JSON:{
  "incident_id": <id>,
  "root_cause": "...",
  "recommended_fix": "...",
  "risk_level": "...",
  "summary": "..."
}

Incident Data (JSON format):{incident_json}"""

# --- 4. Process Incidents in Batch ---
results = []
if not df.empty:
    print("\n--- Starting Batch Processing ---\n")
    for index, row in tqdm(df.iterrows(), total=len(df), desc="Processing Incidents"):
        # Convert incident row to a formatted JSON string for the prompt
        incident_json = json.dumps(row.to_dict(), indent=2)
        # Substitute the incident data into the template
        prompt = PROMPT_TEMPLATE.replace("{incident_json}", incident_json)

        try:
            # Call the Azure OpenAI Chat Completion API
            response = client.chat.completions.create(
                model=DEPLOYMENT_NAME,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                # Using JSON mode helps ensure the output is well-formed JSON
                response_format={"type": "json_object"}
            )

            # Access the content and parse the JSON
            # Using dot notation for object attribute access: .message.content
            json_output = json.loads(response.choices[0].message.content)
            results.append(json_output)

        except json.JSONDecodeError:
            print(f"\n Failed to parse JSON response for incident {row.get('id')}. Raw content: {response.choices[0].message.content[:100]}...")
        except Exception as e:
            print(f"\n An API or connection error occurred for incident {row.get('id')}: {e}")

# --- 5. Save Final Output ---
output_file = "incident_llm_enriched.json"

if results:
    with open(output_file, "w") as f:
        json.dump(results, f, indent=2)

    print(f"\n Processing complete. {len(results)} records saved to: {output_file}")
else:
    print("\nNo results to save.")