{ "cells": [  {   "cell_type": "markdown",   "metadata": {},   "source": [    "# Batch Incident Processing with Azure OpenAI\n",    "\n",    "This notebook takes a dataset of incidents and processes them in batch using Azure OpenAI models.\n",    "\n",    "**Core tasks include:**\n",    "- Loading incident data (NDJSON / JSON)\n",    "- Calling Azure OpenAI (GPT-4o mini / GPT-4 Turbo)\n",    "- Extracting Problem → Cause → Action mappings\n",    "- Saving structured output"   ]  },  {   "cell_type": "markdown",   "metadata": {},   "source": [    "## 1. Set Up Azure OpenAI Connection"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "from openai import AzureOpenAI\n",    "import pandas as pd\n",    "import json\n",    "from tqdm import tqdm\n",    "\n",    "# Fill with your values\n",    "AZURE_ENDPOINT = \"<your-endpoint>\" # e.g. https://xyz.openai.azure.com/\n",    "AZURE_API_KEY = \"<your-api-key>\"\n",    "DEPLOYMENT_NAME = \"<your-deployment>\" # gpt-4o-mini, gpt-4o, etc.\n",    "\n",    "client = AzureOpenAI(\n",    " azure_endpoint=AZURE_ENDPOINT,\n",    " api_key=AZURE_API_KEY,\n",    " api_version=\"2024-02-15-preview\"\n",    ")"   ]  },  {   "cell_type": "markdown",   "metadata": {},   "source": [    "## 2. Load Incident Dataset"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "# Load NDJSON file (one JSON per line)\n",    "input_file = \"incidents.ndjson\" # update if needed\n",    "\n",    "records = []\n",    "with open(input_file, \"r\") as f:\n",    " for line in f:\n",    " if line.strip():\n",    " records.append(json.loads(line))\n",    "\n",    "df = pd.DataFrame(records)\n",    "df.head()"   ]  },  {   "cell_type": "markdown",   "metadata": {},   "source": [    "## 3. Define LLM Prompt Template"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "PROMPT_TEMPLATE = \"\"\"\n",    "You are an SRE/Operations expert.\n",    "Analyze the incident below and extract:\n",    "1. Root Cause (precise)\n",    "2. Recommended Fix (actionable)\n",    "3. Risk of Reoccurrence (Low/Medium/High)\n",    "4. Summary in 1 short sentence.\n",    "\n",    "Respond ONLY in JSON:\n",    "{\n",    " \"incident_id\": <id>,\n",    " \"root_cause\": \"...\",\n",    " \"recommended_fix\": \"...\",\n",    " \"risk_level\": \"...\",\n",    " \"summary\": \"...\"\n",    "}\n",    "\n",    "Incident:\n",    "{incident_json}\n",    "\"\"\""   ]  },  {   "cell_type": "markdown",   "metadata": {},   "source": [    "## 4. Process Incidents in Batch"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "results = []\n",    "\n",    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",    " incident_json = json.dumps(row.to_dict(), indent=2)\n",    " prompt = PROMPT_TEMPLATE.replace(\"{incident_json}\", incident_json)\n",    "\n",    " response = client.chat.completions.create(\n",    " model=DEPLOYMENT_NAME,\n",    " messages=[{\"role\": \"user\", \"content\": prompt}],\n",    " temperature=0.1\n",    " )\n",    "\n",    " try:\n",    " json_output = json.loads(response.choices[0].message[\"content\"])\n",    " results.append(json_output)\n",    " except:\n",    " print(\" Failed to parse JSON for incident\", row.get(\"id\"))"   ]  },  {   "cell_type": "markdown",   "metadata": {},   "source": [    "## 5. Save Final Output"   ]  },  {   "cell_type": "code",   "execution_count": null,   "metadata": {},   "outputs": [],   "source": [    "output_file = \"incident_llm_enriched.json\"\n",    "\n",    "with open(output_file, \"w\") as f:\n",    " json.dump(results, f, indent=2)\n",    "\n",    "output_file"   ]  } ], "metadata": {  "kernelspec": {   "display_name": "Python 3",   "language": "python",   "name": "python3"  },  "language_info": {   "name": "python"  } }, "nbformat": 4, "nbformat_minor": 2}